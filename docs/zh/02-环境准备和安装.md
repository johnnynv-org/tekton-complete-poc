# 02 - 环境准备和安装

本文档详细说明从零开始搭建完整的CI/CD环境，包括Kubernetes集群、Tekton组件和GitHub Runner配置。

## 环境要求

### 基础设施
- **Kubernetes集群**: 单节点或多节点均可，版本>=1.24
- **网络环境**: 集群节点能访问GitHub.com
- **存储**: 支持动态PV provisioning
- **自宿主机器**: 能访问Kubernetes集群内网的机器作为GitHub Runner

### 工具要求
- kubectl (版本匹配Kubernetes)
- curl
- git

## 步骤1：Kubernetes集群准备

### 1.1 验证集群状态

请在k8s环境中执行：

```bash
# 检查集群状态
kubectl cluster-info
kubectl get nodes
kubectl version
```

预期输出示例：
```
Kubernetes control plane is running at https://10.117.3.193:6443
NAME        STATUS   ROLES           AGE     VERSION
ipp1-1877   Ready    control-plane   3h25m   v1.30.14
```

### 1.2 检查必要组件

```bash
# 检查存储类
kubectl get storageclass

# 检查ingress controller
kubectl get pods -n ingress-nginx
```

如果没有ingress-nginx，需要先安装。

## 步骤2：安装Tekton组件

### 2.1 安装Tekton Pipelines

```bash
# 安装核心Pipeline组件
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml

# 等待组件启动
kubectl get pods -n tekton-pipelines
```

### 2.2 安装Tekton Triggers

```bash
# 安装Triggers组件
kubectl apply --filename https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml

# 安装Core Interceptors（重要！EventListener需要这些组件）
kubectl apply --filename https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.yaml

# 验证安装
kubectl get pods -n tekton-pipelines | grep trigger
kubectl get pods -n tekton-pipelines | grep interceptor
kubectl get clusterinterceptor
```

**预期输出：**
```
tekton-triggers-controller-xxx                 1/1     Running
tekton-triggers-webhook-xxx                    1/1     Running  
tekton-triggers-core-interceptors-xxx          1/1     Running

NAME        AGE
bitbucket   1m
cel         1m  
github      1m
gitlab      1m
slack       1m
```

### 2.3 安装Tekton Dashboard

```bash
# 安装Dashboard
kubectl apply --filename https://storage.googleapis.com/tekton-releases/dashboard/latest/release.yaml

# 检查Dashboard pod
kubectl get pods -n tekton-pipelines | grep dashboard
```

### 2.4 配置Dashboard访问

获取当前IP地址：
```bash
hostname -I | awk '{print $1}'
```

创建Ingress配置文件 `tekton-dashboard-ingress.yaml`：

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tekton-dashboard
  namespace: tekton-pipelines
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  rules:
  - host: tekton.<你的IP>.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: tekton-dashboard
            port:
              number: 9097
```

应用配置：
```bash
kubectl apply -f tekton-dashboard-ingress.yaml
```

### 2.5 验证Tekton安装

```bash
# 检查所有组件状态
kubectl get pods -n tekton-pipelines
kubectl get pods -n tekton-pipelines-resolvers

# 测试Dashboard访问
curl -s -o /dev/null -w "%{http_code}\n" http://tekton.<你的IP>.nip.io
```

预期所有Pod状态为Running，Dashboard返回HTTP 200。

## 步骤3：配置GitHub Runner

### 3.1 Runner机器要求

**重要：** Runner机器必须能够访问Kubernetes集群的内网服务。

### 3.2 安装kubectl

在Runner机器上执行：

```bash
# 下载kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# 设置权限并安装
chmod +x kubectl
sudo mv kubectl /usr/local/bin/

# 验证安装
kubectl version --client
```

### 3.3 配置kubectl访问

在k8s环境中获取配置：
```bash
kubectl config view --raw --flatten > /tmp/kubeconfig-for-runner
```

将配置复制到Runner机器：
```bash
# 在Runner机器上执行
mkdir -p ~/.kube

# 复制kubeconfig（使用scp或手动复制）
scp root@<k8s-ip>:/tmp/kubeconfig-for-runner ~/.kube/config
chmod 600 ~/.kube/config

# 测试连接
kubectl get nodes
kubectl get pods -n tekton-pipelines
```

### 3.4 注册GitHub Runner

1. 在GitHub仓库中：`Settings` → `Actions` → `Runners` → `New self-hosted runner`
2. 选择Linux x64平台
3. 按照页面提示在Runner机器上执行安装命令
4. 配置Runner名称为：`swqa-gh-runner-poc`
5. 启动Runner服务

验证Runner状态：
```bash
# 检查Runner服务
sudo systemctl status actions.runner.*

# 或查看GitHub页面确认Runner在线
```

## 步骤4：测试环境连通性

### 4.1 网络连通性测试

在Runner机器上测试：

```bash
# 测试DNS解析
nslookup el-github-listener.default.svc.cluster.local

# 测试基本网络连通（稍后在配置EventListener后测试）
# curl -v http://el-github-listener.default.svc.cluster.local:8080
```

### 4.2 权限测试

在Runner机器上测试：

```bash
# 测试k8s访问权限
kubectl get namespaces
kubectl get pods -n tekton-pipelines

# 测试创建资源权限（用于EventListener部署）
kubectl auth can-i create eventlisteners --namespace=default
kubectl auth can-i create triggerbindings --namespace=default
```

## 环境验证清单

安装完成后，确认以下项目：

- [ ] Kubernetes集群运行正常
- [ ] Tekton Pipelines组件全部Running
- [ ] Tekton Triggers组件全部Running  
- [ ] Tekton Dashboard可访问
- [ ] GitHub Runner在线并连接
- [ ] Runner能够访问k8s集群
- [ ] kubectl在Runner上工作正常

## 故障排除

### Tekton组件启动失败

```bash
# 查看具体错误
kubectl describe pod <pod-name> -n tekton-pipelines
kubectl logs <pod-name> -n tekton-pipelines
```

### Runner连接问题

```bash
# 检查Runner日志
sudo journalctl -u actions.runner.* -f

# 检查网络连接
ping <k8s-cluster-ip>
telnet <k8s-cluster-ip> 6443
```

### Dashboard访问问题

```bash
# 检查ingress状态
kubectl get ingress -n tekton-pipelines
kubectl describe ingress tekton-dashboard -n tekton-pipelines

# 检查Service
kubectl get svc tekton-dashboard -n tekton-pipelines
```

## 步骤5：部署项目基础设施

### 5.1 目录结构说明

本项目采用分层配置管理：

```
.tekton/
├── infrastructure/           # 基础设施层（一次性部署）
│   ├── rbac.yaml            # 权限配置
│   ├── eventlistener.yaml   # 事件监听器
│   ├── triggerbinding.yaml  # 参数绑定
│   └── triggertemplate.yaml # 触发模板
└── pipelines/               # 业务逻辑层（版本化部署）
    ├── task-pytest.yaml    # 任务定义
    ├── pipeline.yaml       # 流水线定义
    └── pipelinerun.yaml    # 示例运行（手动测试用）
```

**设计理念：**
- **基础设施层**: 由运维团队一次性部署，跨项目共用
- **业务逻辑层**: 由GitHub Actions自动部署，随代码版本演进

### 5.2 部署基础设施

在k8s环境中执行：

```bash
# 应用RBAC配置
kubectl apply -f .tekton/infrastructure/rbac.yaml

# 应用TriggerBinding
kubectl apply -f .tekton/infrastructure/triggerbinding.yaml

# 应用TriggerTemplate  
kubectl apply -f .tekton/infrastructure/triggertemplate.yaml

# 应用EventListener
kubectl apply -f .tekton/infrastructure/eventlistener.yaml
```

### 5.3 验证基础设施部署

```bash
# 检查EventListener部署状态
kubectl get eventlistener -n default

# 检查EventListener Pod
kubectl get pods -l eventlistener=github-listener -n default

# 检查Service创建
kubectl get svc -l eventlistener=github-listener -n default

# 等待Pod Ready
kubectl wait --for=condition=Ready pod -l eventlistener=github-listener -n default --timeout=60s
```

### 5.4 解决网络连通性问题

**问题背景：** Runner机器通常无法直接访问Kubernetes Service CIDR网段，需要创建NodePort Service来解决网络路由问题。

**创建NodePort Service：**

```bash
# 创建NodePort Service配置
cat > .tekton/infrastructure/eventlistener-nodeport.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: el-github-listener-nodeport
  namespace: tekton-pipelines
  labels:
    app: eventlistener-nodeport
spec:
  type: NodePort
  selector:
    eventlistener: github-listener
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30080
      protocol: TCP
      name: http-listener
EOF

# 应用NodePort Service
kubectl apply -f .tekton/infrastructure/eventlistener-nodeport.yaml

# 验证NodePort Service
kubectl get svc -n tekton-pipelines | grep nodeport
```

**预期输出：**
```
el-github-listener-nodeport         NodePort    10.96.97.5       <none>        8080:30080/TCP
```

### 5.5 测试EventListener连通性

在k8s环境中测试（使用ClusterIP）：

```bash
# 获取EventListener Service的ClusterIP
SERVICE_IP=$(kubectl get svc el-github-listener -n tekton-pipelines -o jsonpath='{.spec.clusterIP}')
echo "EventListener Service IP: $SERVICE_IP"

# 发送测试请求
curl -v -X POST http://$SERVICE_IP:8080 \
  -H "Content-Type: application/json" \
  -H "X-GitHub-Event: ping" \
  -d '{"zen": "test connectivity"}'
```

在Runner机器上测试（使用NodePort）：

```bash
# 获取集群节点IP
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')

# 测试Runner到EventListener的连通性（使用NodePort）
curl -v http://$NODE_IP:30080 \
  -H "Content-Type: application/json" \
  -H "X-GitHub-Event: ping" \
  -d '{"zen": "nodeport test from runner"}'
```

**预期成功输出：**
```
< HTTP/1.1 202 Accepted
< Content-Type: application/json
{"eventListener":"github-listener","namespace":"tekton-pipelines",...}
```

## 下一步

环境准备完成后，请继续：
- [03 - Tekton Triggers配置](./03-Tekton-Triggers配置.md)
